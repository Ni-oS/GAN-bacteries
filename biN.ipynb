{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44782eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e99dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(seq):\n",
    "    \"\"\"\n",
    "    Given a DNA sequence, return its one-hot encoding\n",
    "    \"\"\"\n",
    "    # Make sure seq has only allowed bases\n",
    "    allowed = set(\"ACGUN\")\n",
    "    if not set(seq).issubset(allowed):\n",
    "        invalid = set(seq) - allowed\n",
    "        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n",
    "        \n",
    "    # Dictionary returning one-hot encoding for each nucleotide \n",
    "    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n",
    "             'C':[0.0,1.0,0.0,0.0],\n",
    "             'G':[0.0,0.0,1.0,0.0],\n",
    "             'U':[0.0,0.0,0.0,1.0],\n",
    "             'N':[0.0,0.0,0.0,0.0]}\n",
    "    \n",
    "    # Create array from nucleotide sequence\n",
    "    vec=np.array([nuc_d[x] for x in seq])\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e827a0",
   "metadata": {
    "tags": [
     "example"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode(\"ACGU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c7e06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea97b5",
   "metadata": {},
   "source": [
    "### Загрузка датасета бинов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3938ae48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 :     0 (1878, 2)\n",
      "F2 :     1 (1933, 2)\n",
      "F3 :     2 (2158, 2)\n",
      "F4 :     3 (1798, 2)\n",
      "F5 :     4 (1376, 2)\n",
      "F6 :     5 (1471, 2)\n",
      "F78 :    6 (1078, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('20long.csv') # сразу загружается таблица бинов\n",
    "data[\"label\"] = 0\n",
    "\n",
    "\n",
    "for i in range(len(data.columns)-3):\n",
    "    \n",
    "    data.loc[data[f'F{i+1} probability'] > 0.6, \"label\"] = i\n",
    "    \n",
    "\n",
    "data.loc[data['F7 probability'] > 0.6, \"label\"] = 6 \n",
    "data.loc[data['F8 probability'] > 0.6, \"label\"] = 6\n",
    "data.label = data.label.astype(int)\n",
    "\n",
    "#print(data[data[\"label\"] == 1].shape)\n",
    "\n",
    "prob = data[[\"seq\",\"label\"]]\n",
    "\n",
    "for i in range(7):\n",
    "    pr = f\"F{i+1} :    \"\n",
    "    \n",
    "    if i == 6:\n",
    "        pr = pr = f\"F78 :   \"\n",
    "        \n",
    "    print(pr,i,prob[prob[prob.columns[1]] == i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8970c7f5",
   "metadata": {
    "tags": [
     "split"
    ]
   },
   "outputs": [],
   "source": [
    "def quick_split(df, split_frac=0.8, verbose=False):\n",
    "    '''\n",
    "    Given a df of samples, randomly split indices between\n",
    "    train and test at the desired fraction\n",
    "    '''\n",
    "    cols = df.columns # original columns, use to clean up reindexed cols\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # shuffle indices\n",
    "    idxs = list(range(df.shape[0]))\n",
    "    random.shuffle(idxs)\n",
    "\n",
    "    # split shuffled index list by split_frac\n",
    "    split = int(len(idxs)*split_frac)\n",
    "    train_idxs = idxs[:split]\n",
    "    test_idxs = idxs[split:]\n",
    "    \n",
    "    # split dfs and return\n",
    "    train_df = df[df.index.isin(train_idxs)]\n",
    "    test_df = df[df.index.isin(test_idxs)]\n",
    "        \n",
    "    return train_df[cols], test_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e2da2",
   "metadata": {},
   "source": [
    "### Разделение на train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae21c1f8",
   "metadata": {
    "scrolled": true,
    "tags": [
     "split"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7482, 2)\n",
      "Val: (1871, 2)\n",
      "Test: (2339, 2)\n",
      "Train_dataframe, Class  F1 :     (1172, 2)\n",
      "Train_dataframe, Class  F2 :     (1262, 2)\n",
      "Train_dataframe, Class  F3 :     (1380, 2)\n",
      "Train_dataframe, Class  F4 :     (1199, 2)\n",
      "Train_dataframe, Class  F5 :     (842, 2)\n",
      "Train_dataframe, Class  F6 :     (935, 2)\n",
      "Train_dataframe, Class  F78 :    (692, 2)\n",
      "                          seq  label\n",
      "0     UGAAUUAGGAGGGUAUAGAAAUG      6\n",
      "1     AAUACGAGAGGAGGAAGGCAAUG      6\n",
      "2     CACAUAACUGGAGACACAGCAUG      6\n",
      "3     AUGUAAUAGGGAGGAGAAGAAUG      6\n",
      "4     UUACUACGUGGAGAAAAGAGAUG      6\n",
      "...                       ...    ...\n",
      "9348  AGAAUAACUGGAGGAGGAAAAUG      5\n",
      "9349  GUUAUUAAAAGAGAGCAAACAUG      5\n",
      "9350  UACUCACAGAGAGUAAUGAUAUG      5\n",
      "9351  UGCUACAGAAUAAUUACAAGAUG      5\n",
      "9352  UUAUCAUUGAAGGUAUACAUAUG      5\n",
      "\n",
      "[7482 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "full_train_df, test_df = quick_split(prob)\n",
    "train_df, val_df = quick_split(full_train_df)\n",
    "\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Val:\", val_df.shape)\n",
    "print(\"Test:\", test_df.shape)\n",
    "for i in range(7):\n",
    "    pr = f\"F{i+1} :    \"\n",
    "    \n",
    "    if i == 6:\n",
    "        pr = f\"F78 :   \"\n",
    "        \n",
    "    print(\"Train_dataframe, Class \",pr,train_df[train_df[train_df.columns[1]] == i].shape)\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee54704c",
   "metadata": {},
   "source": [
    "#### Если использовать в таблице бины для фракций 1 2 3 4, то примерно шестая часть записей в каждом датафрейме имеет y_true = 1\n",
    "#### Если используются фракции 2 3 4, то положительных ответов примерно столько же сколько отрицательных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff1fa7",
   "metadata": {},
   "source": [
    "#### Создание датасета OHE - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e05f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDatasetOHE(Dataset):\n",
    "    '''\n",
    "    Dataset for one-hot-encoded sequences\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 seq_col='seq',\n",
    "                 target_col='label'\n",
    "                ):\n",
    "        # +--------------------+\n",
    "        # | Get the X examples |\n",
    "        # +--------------------+\n",
    "        # extract the DNA from the appropriate column in the df\n",
    "        self.seqs = list(df[seq_col].values) # лист всех последовательностей\n",
    "        self.seq_len = len(self.seqs[0])\n",
    "        \n",
    "        # one-hot encode sequences, then stack in a torch tensor\n",
    "        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])\n",
    "        \n",
    "        # +------------------+\n",
    "        # | Get the Y labels |\n",
    "        # +------------------+\n",
    "        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)\n",
    "\n",
    "        \n",
    "    def __len__(self): return len(self.seqs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # Given an index, return a tuple of an X with it's associated Y\n",
    "        # This is called inside DataLoader\n",
    "        seq = self.ohe_seqs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return seq, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ec288",
   "metadata": {},
   "source": [
    "### даталоадер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "366ac6d4",
   "metadata": {
    "tags": [
     "dataloader",
     "data"
    ]
   },
   "outputs": [],
   "source": [
    "def build_dataloaders(train_df,\n",
    "                      test_df,\n",
    "                      seq_col='seq',\n",
    "                      target_col='label',\n",
    "                      batch_size=128,\n",
    "                      shuffle=True\n",
    "                     ):\n",
    "    '''\n",
    "    Given a train and test df with some batch construction\n",
    "    details, put them into custom SeqDatasetOHE() objects. \n",
    "    Give the Datasets to the DataLoaders and return.\n",
    "    '''\n",
    "    \n",
    "    # create Datasets    \n",
    "    train_ds = SeqDatasetOHE(train_df,seq_col=seq_col,target_col=target_col)\n",
    "    test_ds = SeqDatasetOHE(test_df,seq_col=seq_col,target_col=target_col)\n",
    "\n",
    "    # Put DataSets into DataLoaders\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    return train_dl,test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88036d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl = build_dataloaders(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963645f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738f98ad",
   "metadata": {},
   "source": [
    "## Линейная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f3413",
   "metadata": {
    "tags": [
     "model",
     "linear"
    ]
   },
   "outputs": [],
   "source": [
    "# very simple linear model\n",
    "class DNA_Linear(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # the 4 is for our one-hot encoded vector length 4!\n",
    "        self.lin1 = nn.Linear(4*seq_len,1)\n",
    "        \n",
    "        self.S = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # reshape to flatten sequence dimension\n",
    "        x = x.view(x.shape[0],self.seq_len*4)\n",
    "        \n",
    "        # Linear wraps up the weights/bias dot product operations\n",
    "        x = self.lin1(x)\n",
    "        \n",
    "        x = self.S(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "        #return out\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809b630",
   "metadata": {},
   "source": [
    "## Сверточная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db6ff1",
   "metadata": {
    "tags": [
     "model",
     "cnn"
    ]
   },
   "outputs": [],
   "source": [
    "# basic CNN model\n",
    "class DNA_CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_len,\n",
    "                 num_filters=23*4,\n",
    "                 kernel_size=7):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.conv_net = nn.Sequential(\n",
    "            \n",
    "            # 4 is for the 4 nucleotides\n",
    "            nn.Conv1d(in_channels=4, out_channels = num_filters, kernel_size=kernel_size), # 23*4*(7-2+1)\n",
    "            \n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(num_filters*(seq_len-kernel_size+1), 32),\n",
    "            \n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "            nn.Linear(32,1),\n",
    "            \n",
    "            nn.Sigmoid()\n",
    "        ) \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # reshape view to batch_size x 4channel x seq_len\n",
    "        # permute to put channel in correct order\n",
    "        x = x.permute(0,2,1) \n",
    "        \n",
    "        #print(xb.shape)\n",
    "        x = self.conv_net(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac927a59",
   "metadata": {},
   "source": [
    "## small cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f77862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_cnn(nn.Module):\n",
    "    def __init__(self,\n",
    "                seq_len,\n",
    "                block_sizes=[16, 24, 32, 40, 48],\n",
    "                kernel_size=7):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "      \n",
    "        nn_blocks = []\n",
    "      \n",
    "        for in_bs, out_bs in zip([4] + block_sizes, block_sizes):\n",
    "            \n",
    "            block = nn.Sequential(\n",
    "                nn.Conv1d(in_bs, out_bs, kernel_size=kernel_size, padding=1),\n",
    "                nn.Sigmoid(),\n",
    "                nn.BatchNorm1d(out_bs)\n",
    "            )\n",
    "            \n",
    "            nn_blocks.append(block)\n",
    "            \n",
    "        self.conv_net = nn.Sequential(\n",
    "            *nn_blocks,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(block_sizes[-1] * (seq_len + len(block_sizes)*(3-kernel_size)), 7),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reshape view to batch_size x 4channel x seq_len\n",
    "        # permute to put channel in correct order\n",
    "        x = x.permute(0,2,1) \n",
    "        \n",
    "        out = self.conv_net(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065bae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ae9147",
   "metadata": {},
   "source": [
    "### функция потерь FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acedf358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/4\"\"\"\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha = 0.25, gamma = 2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = torch.tensor(gamma, dtype = torch.float32)\n",
    "        self.alpha = alpha\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(input, target, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss) # prevents nans when probability 0\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c44a97",
   "metadata": {},
   "source": [
    "### Лосс-функция "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b364d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4e168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf192685",
   "metadata": {
    "tags": [
     "loss"
    ]
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, x, y, opt=None):\n",
    "    \n",
    "    logits = model(x.float())\n",
    "    \n",
    "    loss = loss_func(logits, y.float())\n",
    "    \n",
    "    if opt is not None: # Вывод для train\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        return loss.item(), len(x)\n",
    "\n",
    "    return loss.item(), len(x), logits # Вывод для val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6177d59",
   "metadata": {},
   "source": [
    "### Train + Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d778c34c",
   "metadata": {
    "tags": [
     "train"
    ]
   },
   "outputs": [],
   "source": [
    "# +--------------------------------+\n",
    "# | Training and fitting functions |\n",
    "# +--------------------------------+\n",
    "\n",
    "def train_step(model, train_dl, loss_func, device, opt):\n",
    "    '''\n",
    "    Execute 1 set of batched training within an epoch\n",
    "    '''\n",
    "    # Set model to Training mode\n",
    "    model.train()\n",
    "    \n",
    "    tl = [] # train losses\n",
    "    ns = [] # batch sizes, n\n",
    "    \n",
    "    \n",
    "    # loop through train DataLoader\n",
    "    for batch in train_dl:\n",
    "        \n",
    "        # take seq, labels\n",
    "        x, y = batch\n",
    "        \n",
    "        # put on GPU\n",
    "        x, y = x.to(device),y.to(device)\n",
    "        \n",
    "        # provide opt so backprop happens\n",
    "        t, n = loss_batch(model, loss_func, x, y, opt=opt)\n",
    "        \n",
    "        # collect train loss and batch sizes\n",
    "        tl.append(t)\n",
    "        ns.append(n)\n",
    "    \n",
    "    # average the losses over all batches    \n",
    "    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4a3e8",
   "metadata": {
    "tags": [
     "val"
    ]
   },
   "outputs": [],
   "source": [
    "def val_step(model, val_dl, loss_func, device):\n",
    "    '''\n",
    "    Execute 1 set of batched validation within an epoch\n",
    "    '''\n",
    "    # Set model to Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vl = [] # val losses\n",
    "        ns = [] # batch sizes, n\n",
    "        \n",
    "        # loop through validation DataLoader\n",
    "        for batch in val_dl:\n",
    "            \n",
    "            # take seq, labels\n",
    "            x, y = batch\n",
    "            \n",
    "            # put on GPU\n",
    "            x, y = x.to(device),y.to(device)\n",
    "\n",
    "            # Do NOT provide opt here, so backprop does not happen\n",
    "            v, n, logits = loss_batch(model, loss_func, x, y)\n",
    "\n",
    "            # collect val loss and batch sizes\n",
    "            vl.append(v)\n",
    "            ns.append(n)\n",
    "            \n",
    "            \n",
    "            y_pred = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            \n",
    "            y_pred_list.extend(y_pred.cpu().numpy())\n",
    "            y_true_list.extend(y.numpy())\n",
    "\n",
    "    # average the losses over all batches\n",
    "    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n",
    "    \n",
    "    #roc_auc = roc_auc_score(y_true = y_true_list, y_score = y_pred_list)\n",
    "    \n",
    "    \n",
    "    return val_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11465f90",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee59074",
   "metadata": {
    "tags": [
     "fit"
    ]
   },
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):\n",
    "    '''\n",
    "    Fit the model params to the training data, eval on unseen data.\n",
    "    Loop for a number of epochs and keep train of train and val losses \n",
    "    along the way\n",
    "    '''\n",
    "    # keep track of losses\n",
    "    train_losses = []    \n",
    "    val_losses = []\n",
    "    \n",
    "    # loop through epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        # take a training step\n",
    "        train_loss = train_step(model,train_dl,loss_func,device,opt)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # take a validation step\n",
    "        val_loss = val_step(model,val_dl,loss_func,device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1a11d",
   "metadata": {},
   "source": [
    "### run_model функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_dl,val_dl,model,device,\n",
    "              lr=0.01, epochs=300, \n",
    "              lossf=None,opt=None\n",
    "             ):\n",
    "    '''\n",
    "    Given train and val DataLoaders and a NN model, fit the mode to the training\n",
    "    data. By default, use MSE loss and an SGD optimizer\n",
    "    '''\n",
    "    # define optimizer\n",
    "    if opt:\n",
    "        optimizer = opt(model.parameters(), lr=lr)\n",
    "    else: # if no opt provided, just use SGD\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    # define loss function\n",
    "    if lossf:\n",
    "        loss_func = lossf\n",
    "    else: # if no loss function provided, just use MSE\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "    # run the training loop\n",
    "    train_losses, val_losses = fit(\n",
    "                                epochs, \n",
    "                                model, \n",
    "                                loss_func, \n",
    "                                optimizer, \n",
    "                                train_dl, \n",
    "                                val_dl, \n",
    "                                device)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00483aa1",
   "metadata": {},
   "source": [
    "### Запускаем Модель Model_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sequence length from the first seq in the df\n",
    "seq_len = len(train_df['seq'].values[0])\n",
    "\n",
    "# create Linear model object\n",
    "model_lin = DNA_Linear(seq_len)\n",
    "model_lin.to(DEVICE) # put on GPU\n",
    "\n",
    "# run the model with default settings!\n",
    "lin_train_losses, lin_val_losses = run_model(\n",
    "    train_dl, \n",
    "    val_dl, \n",
    "    model_lin,\n",
    "    DEVICE,\n",
    "    lossf = FocalLoss(),\n",
    "    opt = torch.optim.AdamW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74951af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_loss_plot(data_label_list,loss_type=\"BCE Loss\",sparse_n=0):\n",
    "    '''\n",
    "    For each train/test loss trajectory, plot loss by epoch\n",
    "    '''\n",
    "    for i,(train_data,test_data,label) in enumerate(data_label_list):    \n",
    "        plt.plot(train_data,linestyle='--',color=f\"C{i}\", label=f\"{label} Train\")\n",
    "        plt.plot(test_data,color=f\"C{i}\", label=f\"{label} Val\",linewidth=3.0)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel(loss_type)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(bbox_to_anchor=(1,1),loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3421d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_data_label = (lin_train_losses,lin_val_losses,\"Lin\")\n",
    "quick_loss_plot([lin_data_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83718283",
   "metadata": {},
   "source": [
    "### Запускаем Модель Model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae05786",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = len(train_df['seq'].values[0])\n",
    "\n",
    "# create Linear model object\n",
    "model_cnn = DNA_CNN(seq_len)\n",
    "model_cnn.to(DEVICE) # put on GPU\n",
    "\n",
    "# run the model with default settings!\n",
    "cnn_train_losses, cnn_val_losses = run_model(\n",
    "    train_dl, \n",
    "    val_dl, \n",
    "    model_cnn,\n",
    "    DEVICE,\n",
    "    lossf = FocalLoss(),\n",
    "    opt = torch.optim.AdamW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c03f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dl:\n",
    "    print(x.shape,y.shape)\n",
    "    break\n",
    "test_df[test_df[\"binary\"] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = len(train_df['seq'].values[0])\n",
    "\n",
    "# create Linear model object\n",
    "model_cnn = small_cnn(seq_len)\n",
    "model_cnn.to(DEVICE) # put on GPU\n",
    "\n",
    "# run the model with default settings!\n",
    "cnn_train_losses, cnn_val_losses = run_model(\n",
    "    train_dl, \n",
    "    val_dl, \n",
    "    model_cnn,\n",
    "    DEVICE,\n",
    "    lossf = FocalLoss(),\n",
    "    opt = torch.optim.AdamW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a640cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data_label = (cnn_train_losses,cnn_val_losses,\"CNN\")\n",
    "quick_loss_plot([lin_data_label,cnn_data_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_seq_pred(model, seqs, oracle):\n",
    "    '''\n",
    "    Given a model and some sequences, get the model's predictions\n",
    "    for those sequences and compare to the oracle (true) output\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame(data = {'seq': [], 'pred': [], 'actual': []})\n",
    "    \n",
    "    for dna in seqs:\n",
    "        s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)# One_Hot_Encode\n",
    "        \n",
    "        pred = model(s.float()) # model predicted value\n",
    "        \n",
    "        actual = oracle[dna] # actual value\n",
    "        \n",
    "        df = df.append({'seq': dna, 'pred': pred.item(), 'actual': actual}, ignore_index = True)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde30ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_roc(df,model):\n",
    "    y_true=df[\"actual\"]\n",
    "    y_score=df[\"pred\"]\n",
    "\n",
    "    fpr, tpr, treshold = roc_curve(y_true, y_score)\n",
    "    auc_score = roc_auc_score(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(\"roc_auc_scor: \",auc_score)\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "         label='ROC кривая (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC-кривая {model}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f3580",
   "metadata": {},
   "source": [
    "### ROC_AUC на валидации linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598827aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = dict(val_df[['seq','binary']].values)\n",
    "\n",
    "\n",
    "df1 = quick_seq_pred(model_lin, oracle.keys(), oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_roc(df1,\"model_lin_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0ae7a",
   "metadata": {},
   "source": [
    "### ROC_AUC на тесте linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = dict(test_df[['seq','binary']].values)\n",
    "\n",
    "df2 = quick_seq_pred(model_lin, oracle.keys(), oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee60d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_roc(df2,\"model_lin_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae959f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.distplot(df2['pred'])\n",
    "fig = sns_plot.get_figure()\n",
    "h = df2['pred'].hist()\n",
    "fig = h.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6181338c",
   "metadata": {},
   "source": [
    "### ROC_AUC на валидации cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1406314",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = dict(val_df[['seq','binary']].values)\n",
    "\n",
    "df3 = quick_seq_pred(model_cnn, oracle.keys(), oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_roc(df3,\"model_cnn_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8893b2",
   "metadata": {},
   "source": [
    "### ROC_AUC на тесте cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8210bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = dict(test_df[['seq','binary']].values)\n",
    "\n",
    "df4 = quick_seq_pred(model_cnn, oracle.keys(), oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_roc(df4,\"model_cnn_test\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
