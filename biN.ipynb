{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44782eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70ea925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = pd.read_csv('20long.csv')\n",
    "prob[\"binary\"] = prob[\"F3 probability\"].round()\n",
    "prob.binary = prob.binary.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "36707331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>F1 probability</th>\n",
       "      <th>F2 probability</th>\n",
       "      <th>F3 probability</th>\n",
       "      <th>F4 probability</th>\n",
       "      <th>F5 probability</th>\n",
       "      <th>F6 probability</th>\n",
       "      <th>F7 probability</th>\n",
       "      <th>F8 probability</th>\n",
       "      <th>binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [seq, F1 probability, F2 probability, F3 probability, F4 probability, F5 probability, F6 probability, F7 probability, F8 probability, binary]\n",
       "Index: []"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[(prob[\"F3 probability\"] < 0) | (prob[\"F3 probability\"] > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e99dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(seq):\n",
    "    \"\"\"\n",
    "    Given a DNA sequence, return its one-hot encoding\n",
    "    \"\"\"\n",
    "    # Make sure seq has only allowed bases\n",
    "    allowed = set(\"ACUGN\")\n",
    "    if not set(seq).issubset(allowed):\n",
    "        invalid = set(seq) - allowed\n",
    "        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n",
    "        \n",
    "    # Dictionary returning one-hot encoding for each nucleotide \n",
    "    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n",
    "             'C':[0.0,1.0,0.0,0.0],\n",
    "             'G':[0.0,0.0,1.0,0.0],\n",
    "             'U':[0.0,0.0,0.0,1.0],\n",
    "             'N':[0.0,0.0,0.0,0.0]}\n",
    "    \n",
    "    # Create array from nucleotide sequence\n",
    "    vec=np.array([nuc_d[x] for x in seq])\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5001d983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode((prob[\"seq\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0c7e06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaaf104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8970c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_split(df, split_frac=0.8, verbose=False):\n",
    "    '''\n",
    "    Given a df of samples, randomly split indices between\n",
    "    train and test at the desired fraction\n",
    "    '''\n",
    "    cols = df.columns # original columns, use to clean up reindexed cols\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # shuffle indices\n",
    "    idxs = list(range(df.shape[0]))\n",
    "    random.shuffle(idxs)\n",
    "\n",
    "    # split shuffled index list by split_frac\n",
    "    split = int(len(idxs)*split_frac)\n",
    "    train_idxs = idxs[:split]\n",
    "    test_idxs = idxs[split:]\n",
    "    \n",
    "    # split dfs and return\n",
    "    train_df = df[df.index.isin(train_idxs)]\n",
    "    test_df = df[df.index.isin(test_idxs)]\n",
    "        \n",
    "    return train_df[cols], test_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "741aa106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7482, 10)\n",
      "Val: (1871, 10)\n",
      "Test: (2339, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>F1 probability</th>\n",
       "      <th>F2 probability</th>\n",
       "      <th>F3 probability</th>\n",
       "      <th>F4 probability</th>\n",
       "      <th>F5 probability</th>\n",
       "      <th>F6 probability</th>\n",
       "      <th>F7 probability</th>\n",
       "      <th>F8 probability</th>\n",
       "      <th>binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UGAAUUAGGAGGGUAUAGAAAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073984</td>\n",
       "      <td>0.926016</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAUACGAGAGGAGGAAGGCAAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UUACUACGUGGAGAAAAGAGAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016631</td>\n",
       "      <td>0.983369</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AGAGAUAGAGGAGGAUUAAAAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AUAUGGUGGAGGAAAUAGUCAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.987325</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GUAUGAAGAGGAGAAAAGGUAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GUACGAAGAGGAGGAAUAUAAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033742</td>\n",
       "      <td>0.966258</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AGAAGAGGAGGACUGAGAUGAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ACAAAGAAUGGAGGUACGUUAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060905</td>\n",
       "      <td>0.939095</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GAAAGUGAUGAGGCAUAGGUAUG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        seq  F1 probability  F2 probability  F3 probability  \\\n",
       "0   UGAAUUAGGAGGGUAUAGAAAUG             0.0             0.0             0.0   \n",
       "1   AAUACGAGAGGAGGAAGGCAAUG             0.0             0.0             0.0   \n",
       "4   UUACUACGUGGAGAAAAGAGAUG             0.0             0.0             0.0   \n",
       "5   AGAGAUAGAGGAGGAUUAAAAUG             0.0             0.0             0.0   \n",
       "6   AUAUGGUGGAGGAAAUAGUCAUG             0.0             0.0             0.0   \n",
       "8   GUAUGAAGAGGAGAAAAGGUAUG             0.0             0.0             0.0   \n",
       "9   GUACGAAGAGGAGGAAUAUAAUG             0.0             0.0             0.0   \n",
       "11  AGAAGAGGAGGACUGAGAUGAUG             0.0             0.0             0.0   \n",
       "12  ACAAAGAAUGGAGGUACGUUAUG             0.0             0.0             0.0   \n",
       "14  GAAAGUGAUGAGGCAUAGGUAUG             0.0             0.0             0.0   \n",
       "\n",
       "    F4 probability  F5 probability  F6 probability  F7 probability  \\\n",
       "0         0.000000             0.0        0.000000        0.073984   \n",
       "1         0.000000             0.0        0.000000        0.000000   \n",
       "4         0.000000             0.0        0.000000        0.016631   \n",
       "5         0.000606             0.0        0.000000        0.000000   \n",
       "6         0.000000             0.0        0.000000        0.012675   \n",
       "8         0.000000             0.0        0.000000        0.000000   \n",
       "9         0.000000             0.0        0.000000        0.033742   \n",
       "11        0.000000             0.0        0.000873        0.000000   \n",
       "12        0.000000             0.0        0.000000        0.060905   \n",
       "14        0.000000             0.0        0.000000        0.000000   \n",
       "\n",
       "    F8 probability  binary  \n",
       "0         0.926016     0.0  \n",
       "1         1.000000     0.0  \n",
       "4         0.983369     0.0  \n",
       "5         0.999394     0.0  \n",
       "6         0.987325     0.0  \n",
       "8         1.000000     0.0  \n",
       "9         0.966258     0.0  \n",
       "11        0.999127     0.0  \n",
       "12        0.939095     0.0  \n",
       "14        1.000000     0.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df, test_df = quick_split(prob)\n",
    "train_df, val_df = quick_split(full_train_df)\n",
    "\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Val:\", val_df.shape)\n",
    "print(\"Test:\", test_df.shape)\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2b6e6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11692 entries, 0 to 11691\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   seq             11692 non-null  object \n",
      " 1   F1 probability  11692 non-null  float64\n",
      " 2   F2 probability  11692 non-null  float64\n",
      " 3   F3 probability  11692 non-null  float64\n",
      " 4   F4 probability  11692 non-null  float64\n",
      " 5   F5 probability  11692 non-null  float64\n",
      " 6   F6 probability  11692 non-null  float64\n",
      " 7   F7 probability  11692 non-null  float64\n",
      " 8   F8 probability  11692 non-null  float64\n",
      " 9   binary          11692 non-null  int32  \n",
      "dtypes: float64(8), int32(1), object(1)\n",
      "memory usage: 867.9+ KB\n"
     ]
    }
   ],
   "source": [
    "prob.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "87e05f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDatasetOHE(Dataset):\n",
    "    '''\n",
    "    Dataset for one-hot-encoded sequences\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 seq_col='seq',\n",
    "                 target_col='F3 probability'\n",
    "                ):\n",
    "        # +--------------------+\n",
    "        # | Get the X examples |\n",
    "        # +--------------------+\n",
    "        # extract the DNA from the appropriate column in the df\n",
    "        self.seqs = list(df[seq_col].values)\n",
    "        self.seq_len = len(self.seqs[0])\n",
    "        \n",
    "        # one-hot encode sequences, then stack in a torch tensor\n",
    "        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])\n",
    "    \n",
    "        # +------------------+\n",
    "        # | Get the Y labels |\n",
    "        # +------------------+\n",
    "        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self): return len(self.seqs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # Given an index, return a tuple of an X with it's associated Y\n",
    "        # This is called inside DataLoader\n",
    "        seq = self.ohe_seqs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return seq, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "366ac6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloaders(train_df,\n",
    "                      test_df,\n",
    "                      seq_col='seq',\n",
    "                      target_col='F3 probability',\n",
    "                      batch_size=128,\n",
    "                      shuffle=True\n",
    "                     ):\n",
    "    '''\n",
    "    Given a train and test df with some batch construction\n",
    "    details, put them into custom SeqDatasetOHE() objects. \n",
    "    Give the Datasets to the DataLoaders and return.\n",
    "    '''\n",
    "    \n",
    "    # create Datasets    \n",
    "    train_ds = SeqDatasetOHE(train_df,seq_col=seq_col,target_col=target_col)\n",
    "    test_ds = SeqDatasetOHE(test_df,seq_col=seq_col,target_col=target_col)\n",
    "\n",
    "    # Put DataSets into DataLoaders\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    return train_dl,test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cdb51951",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl = build_dataloaders(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65db6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very simple linear model\n",
    "class DNA_Linear(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        # the 4 is for our one-hot encoded vector length 4!\n",
    "        self.lin = nn.Linear(4*seq_len, 1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # reshape to flatten sequence dimension\n",
    "        xb = xb.view(xb.shape[0],self.seq_len*4)\n",
    "        # Linear wraps up the weights/bias dot product operations\n",
    "        out = self.lin(xb)\n",
    "        return out\n",
    "\n",
    "    \n",
    "# basic CNN model\n",
    "class DNA_CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_len,\n",
    "                 num_filters=32,\n",
    "                 kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.conv_net = nn.Sequential(\n",
    "            # 4 is for the 4 nucleotides\n",
    "            nn.Conv1d(4, num_filters, kernel_size=kernel_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_filters*(seq_len-kernel_size+1), 1)\n",
    "        ) \n",
    "    def forward(self, xb):\n",
    "        # reshape view to batch_size x 4channel x seq_len\n",
    "        # permute to put channel in correct order\n",
    "        xb = xb.permute(0,2,1) \n",
    "        \n",
    "        #print(xb.shape)\n",
    "        out = self.conv_net(xb)\n",
    "        return out\n",
    "    \n",
    "    # __FOOTNOTE 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d778c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# +--------------------------------+\n",
    "# | Training and fitting functions |\n",
    "# +--------------------------------+\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None,verbose=False):\n",
    "    '''\n",
    "    Apply loss function to a batch of inputs. If no optimizer\n",
    "    is provided, skip the back prop step.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('loss batch ****')\n",
    "        print(\"xb shape:\",xb.shape)\n",
    "        print(\"yb shape:\",yb.shape)\n",
    "        print(\"yb shape:\",yb.squeeze(1).shape)\n",
    "        #print(\"yb\",yb)\n",
    "\n",
    "    # get the batch output from the model given your input batch \n",
    "    # ** This is the model's prediction for the y labels! **\n",
    "    xb_out = model(xb.float())\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"model out pre loss\", xb_out.shape)\n",
    "        #print('xb_out', xb_out)\n",
    "        print(\"xb_out:\",xb_out.shape)\n",
    "        print(\"yb:\",yb.shape)\n",
    "        print(\"yb.long:\",yb.long().shape)\n",
    "        print(\"yb.long.squeeze(1):\",yb.long().squeeze(1).shape)\n",
    "    \n",
    "    #loss = loss_func(xb_out, yb.float()) # for MSE/regression\n",
    "    # __FOOTNOTE 2__\n",
    "    loss = loss_func(xb_out, yb.long().squeeze(1)) # for BCE/classification\n",
    "    \n",
    "    if opt is not None: # if opt\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "    \n",
    "def train_step(model, train_dl, loss_func, device, opt):\n",
    "    '''\n",
    "    Execute 1 set of batched training within an epoch\n",
    "    '''\n",
    "    # Set model to Training mode\n",
    "    model.train()\n",
    "    tl = [] # train losses\n",
    "    ns = [] # batch sizes, n\n",
    "    \n",
    "    # loop through train DataLoader\n",
    "    for xb, yb in train_dl:\n",
    "        # put on GPU\n",
    "        xb, yb = xb.to(device),yb.to(device)\n",
    "        \n",
    "        # provide opt so backprop happens\n",
    "        t, n = loss_batch(model, loss_func, xb, yb, opt=opt,verbose = True)\n",
    "        \n",
    "        # collect train loss and batch sizes\n",
    "        tl.append(t)\n",
    "        ns.append(n)\n",
    "    \n",
    "    # average the losses over all batches    \n",
    "    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def val_step(model, val_dl, loss_func, device):\n",
    "    '''\n",
    "    Execute 1 set of batched validation within an epoch\n",
    "    '''\n",
    "    # Set model to Evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vl = [] # val losses\n",
    "        ns = [] # batch sizes, n\n",
    "        \n",
    "        # loop through validation DataLoader\n",
    "        for xb, yb in val_dl:\n",
    "            # put on GPU\n",
    "            xb, yb = xb.to(device),yb.to(device)\n",
    "\n",
    "            # Do NOT provide opt here, so backprop does not happen\n",
    "            v, n = loss_batch(model, loss_func, xb, yb,verbose =True)\n",
    "\n",
    "            # collect val loss and batch sizes\n",
    "            vl.append(v)\n",
    "            ns.append(n)\n",
    "\n",
    "    # average the losses over all batches\n",
    "    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n",
    "    \n",
    "    return val_loss\n",
    "    \n",
    "def fit(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):\n",
    "    '''\n",
    "    Fit the model params to the training data, eval on unseen data.\n",
    "    Loop for a number of epochs and keep train of train and val losses \n",
    "    along the way\n",
    "    '''\n",
    "    # keep track of losses\n",
    "    train_losses = []    \n",
    "    val_losses = []\n",
    "    \n",
    "    # loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        # take a training step\n",
    "        train_loss = train_step(model,train_dl,loss_func,device,opt)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # take a validation step\n",
    "        val_loss = val_step(model,val_dl,loss_func,device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def run_model(train_dl,val_dl,model,device,\n",
    "              lr=0.01, epochs=50, \n",
    "              lossf=None,opt=None\n",
    "             ):\n",
    "    '''\n",
    "    Given train and val DataLoaders and a NN model, fit the mode to the training\n",
    "    data. By default, use MSE loss and an SGD optimizer\n",
    "    '''\n",
    "    # define optimizer\n",
    "    if opt:\n",
    "        optimizer = opt\n",
    "    else: # if no opt provided, just use SGD\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    # define loss function\n",
    "    if lossf:\n",
    "        loss_func = lossf\n",
    "    else: # if no loss function provided, just use MSE\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # run the training loop\n",
    "    train_losses, val_losses = fit(\n",
    "                                epochs, \n",
    "                                model, \n",
    "                                loss_func, \n",
    "                                optimizer, \n",
    "                                train_dl, \n",
    "                                val_dl, \n",
    "                                device)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "748a846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss batch ****\n",
      "xb shape: torch.Size([128, 23, 4])\n",
      "yb shape: torch.Size([128, 1])\n",
      "yb shape: torch.Size([128])\n",
      "model out pre loss torch.Size([128, 1])\n",
      "xb_out: torch.Size([128, 1])\n",
      "yb: torch.Size([128, 1])\n",
      "yb.long: torch.Size([128, 1])\n",
      "yb.long.squeeze(1): torch.Size([128])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-b12c2773629f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# run the model with default settings!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m lin_train_losses, lin_val_losses = run_model(\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mval_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-21f535b3e1ba>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(train_dl, val_dl, model, device, lr, epochs, lossf, opt)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;31m# run the training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     train_losses, val_losses = fit(\n\u001b[0m\u001b[0;32m    140\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                                 \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-21f535b3e1ba>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(epochs, model, loss_func, opt, train_dl, val_dl, device, patience)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;31m# take a training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-21f535b3e1ba>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, train_dl, loss_func, device, opt)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# provide opt so backprop happens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# collect train loss and batch sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-21f535b3e1ba>\u001b[0m in \u001b[0;36mloss_batch\u001b[1;34m(model, loss_func, xb, yb, opt, verbose)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#loss = loss_func(xb_out, yb.float()) # for MSE/regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# __FOOTNOTE 2__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# for BCE/classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# if opt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1163\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1164\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2994\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2996\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 1 is out of bounds."
     ]
    }
   ],
   "source": [
    "# get the sequence length from the first seq in the df\n",
    "seq_len = len(train_df['seq'].values[0])\n",
    "\n",
    "# create Linear model object\n",
    "model_lin = DNA_Linear(seq_len)\n",
    "model_lin.to(DEVICE) # put on GPU\n",
    "\n",
    "# run the model with default settings!\n",
    "lin_train_losses, lin_val_losses = run_model(\n",
    "    train_dl, \n",
    "    val_dl, \n",
    "    model_lin,\n",
    "    DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f621280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_loss_plot(data_label_list,loss_type=\"MSE Loss\",sparse_n=0):\n",
    "    '''\n",
    "    For each train/test loss trajectory, plot loss by epoch\n",
    "    '''\n",
    "    for i,(train_data,test_data,label) in enumerate(data_label_list):    \n",
    "        plt.plot(train_data,linestyle='--',color=f\"C{i}\", label=f\"{label} Train\")\n",
    "        plt.plot(test_data,color=f\"C{i}\", label=f\"{label} Val\",linewidth=3.0)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel(loss_type)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(bbox_to_anchor=(1,1),loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3421d6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lin_data_label = (lin_train_losses,lin_val_losses,\"Lin\")\n",
    "quick_loss_plot([lin_data_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abbe712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0 | train loss: 0.151 | val loss: 0.157\n",
      "E1 | train loss: 0.148 | val loss: 0.155\n",
      "E2 | train loss: 0.147 | val loss: 0.154\n",
      "E3 | train loss: 0.146 | val loss: 0.153\n",
      "E4 | train loss: 0.145 | val loss: 0.153\n",
      "E5 | train loss: 0.145 | val loss: 0.152\n",
      "E6 | train loss: 0.144 | val loss: 0.151\n",
      "E7 | train loss: 0.144 | val loss: 0.151\n",
      "E8 | train loss: 0.144 | val loss: 0.151\n",
      "E9 | train loss: 0.144 | val loss: 0.152\n",
      "E10 | train loss: 0.143 | val loss: 0.152\n",
      "E11 | train loss: 0.143 | val loss: 0.151\n",
      "E12 | train loss: 0.143 | val loss: 0.151\n",
      "E13 | train loss: 0.143 | val loss: 0.150\n",
      "E14 | train loss: 0.143 | val loss: 0.150\n",
      "E15 | train loss: 0.143 | val loss: 0.150\n",
      "E16 | train loss: 0.142 | val loss: 0.151\n",
      "E17 | train loss: 0.143 | val loss: 0.150\n",
      "E18 | train loss: 0.142 | val loss: 0.150\n",
      "E19 | train loss: 0.142 | val loss: 0.150\n",
      "E20 | train loss: 0.142 | val loss: 0.150\n",
      "E21 | train loss: 0.142 | val loss: 0.150\n",
      "E22 | train loss: 0.142 | val loss: 0.150\n",
      "E23 | train loss: 0.141 | val loss: 0.150\n",
      "E24 | train loss: 0.142 | val loss: 0.150\n",
      "E25 | train loss: 0.141 | val loss: 0.153\n",
      "E26 | train loss: 0.142 | val loss: 0.150\n",
      "E27 | train loss: 0.141 | val loss: 0.150\n",
      "E28 | train loss: 0.141 | val loss: 0.152\n",
      "E29 | train loss: 0.141 | val loss: 0.150\n",
      "E30 | train loss: 0.141 | val loss: 0.150\n",
      "E31 | train loss: 0.141 | val loss: 0.150\n",
      "E32 | train loss: 0.141 | val loss: 0.151\n",
      "E33 | train loss: 0.141 | val loss: 0.150\n",
      "E34 | train loss: 0.141 | val loss: 0.151\n",
      "E35 | train loss: 0.141 | val loss: 0.150\n",
      "E36 | train loss: 0.141 | val loss: 0.150\n",
      "E37 | train loss: 0.141 | val loss: 0.150\n",
      "E38 | train loss: 0.140 | val loss: 0.150\n",
      "E39 | train loss: 0.140 | val loss: 0.150\n",
      "E40 | train loss: 0.140 | val loss: 0.153\n",
      "E41 | train loss: 0.140 | val loss: 0.150\n",
      "E42 | train loss: 0.140 | val loss: 0.151\n",
      "E43 | train loss: 0.140 | val loss: 0.151\n",
      "E44 | train loss: 0.140 | val loss: 0.150\n",
      "E45 | train loss: 0.140 | val loss: 0.150\n",
      "E46 | train loss: 0.140 | val loss: 0.151\n",
      "E47 | train loss: 0.140 | val loss: 0.150\n",
      "E48 | train loss: 0.140 | val loss: 0.150\n",
      "E49 | train loss: 0.140 | val loss: 0.150\n"
     ]
    }
   ],
   "source": [
    "seq_len = len(train_df['seq'].values[0])\n",
    "\n",
    "# create Linear model object\n",
    "model_cnn = DNA_CNN(seq_len)\n",
    "model_cnn.to(DEVICE) # put on GPU\n",
    "\n",
    "# run the model with default settings!\n",
    "cnn_train_losses, cnn_val_losses = run_model(\n",
    "    train_dl, \n",
    "    val_dl, \n",
    "    model_cnn,\n",
    "    DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a640cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_data_label = (cnn_train_losses,cnn_val_losses,\"CNN\")\n",
    "quick_loss_plot([lin_data_label,cnn_data_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8e39990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = dict(test_df[['seq','F3 probability']].values)\n",
    "\n",
    "def quick_seq_pred(model, seqs, oracle):\n",
    "    '''\n",
    "    Given a model and some sequences, get the model's predictions\n",
    "    for those sequences and compare to the oracle (true) output\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame(data = {'seq': [], 'pred': [], 'actual': [], \"diff\": []})\n",
    "    for dna in seqs:\n",
    "        s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n",
    "        pred = model(s.float())\n",
    "        actual = oracle[dna]\n",
    "        diff = pred.item() - actual\n",
    "        #print(f\"{dna}: pred:{pred.item():.3f} actual:{actual:.3f} ({diff:.3f})\")\n",
    "        #df2 = pd.DataFrame(data = {'seq': [dna], 'pred': [pred.item()], 'actual': [actual]})\n",
    "        df = df.append({'seq': dna, 'pred': pred.item(), 'actual': actual, \"diff\": diff}, ignore_index = True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def quick_8mer_pred(model, oracle):\n",
    "\n",
    "    for seqs in oracle.keys():\n",
    "        quick_seq_pred(model, oracle, oracle)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "598827aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = quick_seq_pred(model_lin, oracle.keys(), oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9fdbfaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>pred</th>\n",
       "      <th>actual</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAUACGAGAGGAGGAAGGCAAUG</td>\n",
       "      <td>0.066782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UAAUGAGUACAUCGAAUCGCAUG</td>\n",
       "      <td>0.269617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GGCUUUGUCUACAUCUUUCAAUG</td>\n",
       "      <td>0.225467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUGAGACGGGAGUAAAUAACAUG</td>\n",
       "      <td>0.159584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGAGAGAGGAGAGAGAAAAAAUG</td>\n",
       "      <td>0.092565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AGUUACAGAGGAGAACGAGCAUG</td>\n",
       "      <td>0.086097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GUAAUGACGGUAAUCUCGUAAUG</td>\n",
       "      <td>0.173785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CAAAGAACAGGAGGAACCUUAUG</td>\n",
       "      <td>0.103517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAAAAACGGGAGGAAAGUAAAUG</td>\n",
       "      <td>0.094527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUAUGUAGGAGAGAAAAAAUAUG</td>\n",
       "      <td>0.120179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GAAUACAGAGGAGAAUGAGUAUG</td>\n",
       "      <td>0.124846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ACAUAGAUGAGGAAAAAGAUAUG</td>\n",
       "      <td>0.096451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ACAUAGAUAGGAGAAAAUGAAUG</td>\n",
       "      <td>0.083723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GUUAAACGAGAGGAGGAAACAUG</td>\n",
       "      <td>0.151402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ACUCGAAAGGAGGUAAGAAUAUG</td>\n",
       "      <td>0.119729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CAACUGAGAAGAGGAAAAUUAUG</td>\n",
       "      <td>0.125053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AAACAGAGUAGGAGAAAGAGAUG</td>\n",
       "      <td>0.081540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ACAAAAUGUGGAGGAAACGUAUG</td>\n",
       "      <td>0.070223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AAAGAAAGAGGAGAGUCCCGAUG</td>\n",
       "      <td>0.044037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AUUAAGUAGGAGGAAAAUGAAUG</td>\n",
       "      <td>0.128433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        seq      pred  actual      diff\n",
       "0   AAUACGAGAGGAGGAAGGCAAUG  0.066782     0.0  0.066782\n",
       "1   UAAUGAGUACAUCGAAUCGCAUG  0.269617     0.0  0.269617\n",
       "2   GGCUUUGUCUACAUCUUUCAAUG  0.225467     0.0  0.225467\n",
       "3   AUGAGACGGGAGUAAAUAACAUG  0.159584     0.0  0.159584\n",
       "4   AGAGAGAGGAGAGAGAAAAAAUG  0.092565     0.0  0.092565\n",
       "5   AGUUACAGAGGAGAACGAGCAUG  0.086097     0.0  0.086097\n",
       "6   GUAAUGACGGUAAUCUCGUAAUG  0.173785     0.0  0.173785\n",
       "7   CAAAGAACAGGAGGAACCUUAUG  0.103517     0.0  0.103517\n",
       "8   AAAAAACGGGAGGAAAGUAAAUG  0.094527     0.0  0.094527\n",
       "9   AUAUGUAGGAGAGAAAAAAUAUG  0.120179     0.0  0.120179\n",
       "10  GAAUACAGAGGAGAAUGAGUAUG  0.124846     0.0  0.124846\n",
       "11  ACAUAGAUGAGGAAAAAGAUAUG  0.096451     0.0  0.096451\n",
       "12  ACAUAGAUAGGAGAAAAUGAAUG  0.083723     0.0  0.083723\n",
       "13  GUUAAACGAGAGGAGGAAACAUG  0.151402     0.0  0.151402\n",
       "14  ACUCGAAAGGAGGUAAGAAUAUG  0.119729     0.0  0.119729\n",
       "15  CAACUGAGAAGAGGAAAAUUAUG  0.125053     0.0  0.125053\n",
       "16  AAACAGAGUAGGAGAAAGAGAUG  0.081540     0.0  0.081540\n",
       "17  ACAAAAUGUGGAGGAAACGUAUG  0.070223     0.0  0.070223\n",
       "18  AAAGAAAGAGGAGAGUCCCGAUG  0.044037     0.0  0.044037\n",
       "19  AUUAAGUAGGAGGAAAAUGAAUG  0.128433     0.0  0.128433"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "549f1fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>pred</th>\n",
       "      <th>actual</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>UCGAUCCCUGGAGGUAAAUGAUG</td>\n",
       "      <td>0.093571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>UUAACUACCGGGAGGGCAGGAUG</td>\n",
       "      <td>0.042662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>UUUCAAUACGGAGAAUUCAAAUG</td>\n",
       "      <td>0.174346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>AUAAGACCGGCCAACACGUUAUG</td>\n",
       "      <td>0.083420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>AUCUCGGCAUUACACGACAUAUG</td>\n",
       "      <td>0.240268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>CAAAUACUACACGGCUGAGCAUG</td>\n",
       "      <td>0.143736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>AAACAAAAGUAAAUGGGAUUAUG</td>\n",
       "      <td>0.250463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>AGGGUAAGUCGCUAAUAGUAAUG</td>\n",
       "      <td>0.145499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>AUCUUUGGUAUGUUACUAGCAUG</td>\n",
       "      <td>0.199058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.199058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>GAACAAAGUACACAAACACGAUG</td>\n",
       "      <td>0.166138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>GAUGUGGAUUUUUACACAAUAUG</td>\n",
       "      <td>0.322586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>UGACUCAGAUACAACACGCGAUG</td>\n",
       "      <td>0.126326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>AUUAUCGACCAACGAAUAAGAUG</td>\n",
       "      <td>0.251413</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.748587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>GUGAAGAGGUAUAGAGAAGGAUG</td>\n",
       "      <td>0.233341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.766659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>UAAUCAGAAUAAGUAGAAUAAUG</td>\n",
       "      <td>0.309823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.690177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>UAUUCAAUUAGAAAGAGAACAUG</td>\n",
       "      <td>0.197756</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.802244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>UAGUGGAUGAGUAAAGGCAUAUG</td>\n",
       "      <td>0.216178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.216178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>UUCAAGCCACAGGAGCAUGAAUG</td>\n",
       "      <td>0.201368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>GACAUAGAAAGAGAACAAGAAUG</td>\n",
       "      <td>0.182351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>AUUCAUGGGUGAGAAAAUAUAUG</td>\n",
       "      <td>0.125434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          seq      pred  actual      diff\n",
       "2319  UCGAUCCCUGGAGGUAAAUGAUG  0.093571     0.0  0.093571\n",
       "2320  UUAACUACCGGGAGGGCAGGAUG  0.042662     0.0  0.042662\n",
       "2321  UUUCAAUACGGAGAAUUCAAAUG  0.174346     0.0  0.174346\n",
       "2322  AUAAGACCGGCCAACACGUUAUG  0.083420     0.0  0.083420\n",
       "2323  AUCUCGGCAUUACACGACAUAUG  0.240268     0.0  0.240268\n",
       "2324  CAAAUACUACACGGCUGAGCAUG  0.143736     0.0  0.143736\n",
       "2325  AAACAAAAGUAAAUGGGAUUAUG  0.250463     0.0  0.250463\n",
       "2326  AGGGUAAGUCGCUAAUAGUAAUG  0.145499     0.0  0.145499\n",
       "2327  AUCUUUGGUAUGUUACUAGCAUG  0.199058     0.0  0.199058\n",
       "2328  GAACAAAGUACACAAACACGAUG  0.166138     0.0  0.166138\n",
       "2329  GAUGUGGAUUUUUACACAAUAUG  0.322586     0.0  0.322586\n",
       "2330  UGACUCAGAUACAACACGCGAUG  0.126326     0.0  0.126326\n",
       "2331  AUUAUCGACCAACGAAUAAGAUG  0.251413     1.0 -0.748587\n",
       "2332  GUGAAGAGGUAUAGAGAAGGAUG  0.233341     1.0 -0.766659\n",
       "2333  UAAUCAGAAUAAGUAGAAUAAUG  0.309823     1.0 -0.690177\n",
       "2334  UAUUCAAUUAGAAAGAGAACAUG  0.197756     1.0 -0.802244\n",
       "2335  UAGUGGAUGAGUAAAGGCAUAUG  0.216178     0.0  0.216178\n",
       "2336  UUCAAGCCACAGGAGCAUGAAUG  0.201368     0.0  0.201368\n",
       "2337  GACAUAGAAAGAGAACAAGAAUG  0.182351     0.0  0.182351\n",
       "2338  AUUCAUGGGUGAGAAAAUAUAUG  0.125434     0.0  0.125434"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496018a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
